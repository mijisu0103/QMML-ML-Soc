<div align = "justify">

# QMML Soc 🤖🧠

This repository contains my learning materials and hands-on exercises for QMML society.

<br>

## Environment 👩🏻‍💻
<div align="center">
  <img src="https://img.shields.io/badge/jupyter-%23FA0F00.svg?style=flat-square&logo=jupyter&logoColor=white">&nbsp;&nbsp;&nbsp;&nbsp;
  <img src="http://img.shields.io/badge/Visual%20Studio%20Code-eeeeee.svg?style=flat-square&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAFU0lEQVRIx+1Wa2iWZRi+nuc9fedt7uCnrth0y9QMzVTSLFxhs8iaEFkRCwrKisSCIEgUifpjICFRRgc6iKClleWkxEjRskxHbvPUnC530O3b9p2/932e++7HNw+luP0YCNUDN++fh/t67+u+rvu5BTPjWhyJa3T+e8Dinz3+srkPmw/3w2cZWDqrGD+cTGJhdQggQnNXHOMKAygJ+xBLprFhfzvKwj6MK/RjVNAZs+tE7I2kVTh6fGlkXdBQPyY9I6GYMXOMidrxgZGtWEoBITDpw32nvtx+8GR9S3e69qceY1tLv9zqMeoNgaAQI0y1zGecvuVgx+YjHQMzQ44JWwoAAn8kRM1vveKjxpjYSYT5IwLMAPyWAUOKudsaOz4/1h2fHLAlNAOaGQzANgQUAX2umP3JUb12WMBCAFIAhrycIwEgaJvY2xqr3XP83MaeRLbSMSU0MZQiENHf8tgGkFIiMiSwzxRQGg/Ec7QumdPltiHAlyQKORZ2NHXWr/3++Ka+lFvusw1oYmgipF0FhlCaAU2AYsDL/wcNCdx0Nvf4oa7sx7tPJZ9bsbPr24Ec3Rtx8tf8loEvDv65bE1D83pmDjmmAWZAaUJWERZOjb4/p6p4fcrV0ACI86GvMB0vA/6xLflYT0ZFIo7EiVhu6ieNfRs3NcVf8FvS2n20+5X1u0+s8YhtQwCKCPGsBwLceVUlr94/dczS0pC/NacYRAARoBkgghjSx3taYxOXN5x9/0xCzQ1aAq5mkBD6xgJ5pO30qUmkPGlZJogY8azC+JLQsedrql4805f5pl+E4DkFq/Z08UrfYIuIAb/JrQ114QlXrVgIHL056jwyu9y3M+kSLClgAkZLH01R4aiUdgCep9Gf8TCvumTb2iXT7qsqDXyjLT9koBCKiDUBRHwxeBg+Lg+bmBW12m8v9y26oyK4Ja0YQgCOAQjTgRceDeVE8MydE1Y8NOO6uori4ImEC2TMMAT0BbvpS0JdgerLgG1T4voCG0dOd4dine1hlRqAZgFFDKU1NAl4/mKc5YKSHu2EcopxOsFIugI6r13WfFFYg6GHAWygYyAzZV9rz44Dbb1325leuIkYki7BUwwwQYDx2aHYsk2HB75r7MrMNSUQEhmMDUmkPTJo0E6aBoHBQ1fc2D4w980dLZ+f6klN89kmkjkPMt2bnl6CXYYh0ymPQEwI2QI/t6dufX7bma+bzuWeqh5lYUKhCUX5iZWfYnkvD4vq17cfWd3SGZ8Y9pnoS7soKwh03ndzeX1Nhb9m+ZzSp13FiaRL0AyEbIlYRhW9tffcez+0pd6JOl5RNCjj7nkbYfDLwwCeWVm8pros1HkunsW86rJdL9dOXhTymZtTLmF+ZfDT1XeVPRm2jVgiq6GJYRoCjinxwYHep1/a3rGhrd+daUoBzQw9qGoweEgfr/z6MCQwK+fx9Po5FRuauhKJ75u7URQOYOFNY1FZZOGXM5napV+1f9idVNECR4IBsACSWYVx0TIUjSqGpryeiIGAiZO/PhoZf9WKlWY4prE/WuB7128biZwinH9PGUA8q/Hg5EjDstuKF08YZR9LuJQvh4GgJQFmaOYLiub8vDaH9ToRMxTlqbrSk+jmvb3vsWlFddUlzsF4ji5yeUk3NQMpj1HsQ+eILQKaAE9z87OzSxcvqArtSmQUmAHBDE8zBrIER3J6UhG2vjLDempENxBNDGa0raoZ/ciCGyINyZxCymM4Eql7rsPmukqxeP44WSeA30d8y9TMUITu1XePXfLEraNfu2Ws7+1FFbzggQrxsG1ghzs4RIZU9f8L/b8O+C9rntGn+AUx4QAAAABJRU5ErkJggg==">&nbsp;&nbsp;&nbsp;&nbsp;
  <img src = "https://img.shields.io/badge/Google%20Colab-%23F9A825.svg?style=flat-square&logo=googlecolab&logoColor=white">

</div>

<br>

## Stack 🛠️
<div align="center">

  <img src="https://img.shields.io/badge/python-3670A0?style=flat-square&logo=python&logoColor=white">

</div>

<br>

## Newly Acquired Stack

<div align = "center">
  <img src="https://img.shields.io/badge/TensorFlow-%23FF6F00.svg?style=flat-square&logo=TensorFlow&logoColor=white">&nbsp;&nbsp;&nbsp;&nbsp;
  <img src="https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=flat-square&logo=PyTorch&logoColor=white">&nbsp;&nbsp;&nbsp;&nbsp;
  <img src="https://img.shields.io/badge/Keras-%23D00000.svg?style=flat-square&logo=Keras&logoColor=white">

</div>

<br>

## Lectures and Workshop Overview

### 1. Introduction to Machine Learning
- What is Machine Learning?
- Real-life applications of ML
- Introduction to Python
- Basic mathematical concepts and diagrams

### 2. Simple Linear Regression
- Review of introductory concepts
- Supervised learning and regression problems
- Simple linear regression explained
- Derivatives and partial derivatives in ML

### 3. Multiple Linear Regression
- Feature scaling and normalisation
- Model performance: MSC and R²
- Overfitting and underfitting fundamentals

### 4. Neural Networks I
- Introduction to neural networks
- Network architecture and activation functions
- Forward propagation walkthrough
- Hands-on: Building a simple neural network using NumPy

### 5. Neural Networks II
- Advanced forward pass algorithms
- Sigmoid, Softmax, and categorical cross-entropy loss
- Back-propagation theory and practice
- Hands-on workshop

### 6. Classification
- Classification problem types and real-world use cases
- Common classification algorithms
- Evaluation metrics and loss functions
- Hands-on implementation

### 7. Introduction to Convolutional Neural Networks (CNNs)
- Core CNN components: convolution, pooling, activation
- CNN architectures: LeNet, AlexNet, VGG, ResNet
- Applications in image processing and beyond
- Transfer learning and data augmentation
- Hands-on workshop

### 8. Implementing Large Language Models (LLMs)
- LLM lifecycle: pre-training, fine-tuning, prompting
- Use cases: chatbots, summarisation, code generation
- API integration with OpenAI, Hugging Face, etc.
- Hands-on workshop with mentoring

### 9. Reinforcement Learning: Beating the Market
- Fundamentals of reinforcement learning
- The K-arm bandit problem
- Designing a trading strategy using RL
- Hands-on workshop with mentoring

### 10. Markov Decision Processes (MDPs)
- MDP components: states, actions, rewards, transitions
- Bellman equations and optimality
- Policy/value iteration and evaluation
- Exploration strategies revisited

### 11. RNNs & LSTMs in PyTorch
- Understanding Recurrent Neural Networks
- LSTM architecture and capabilities
- Sequential data applications: time-series, NLP
- Hands-on PyTorch tutorial

### 12. Stock Prediction with LSTMs
- Addressing gradient exploding/vanishing in RNNs
- Why LSTMs excel in time-series prediction
- Live demo: LSTMs applied to stock market forecasting

<br>

## Repository Structure 🌲
```
.
├── Lectures
│   ├── data
│   │   └── timemachine.txt
│   ├── lecture_01
│   │   ├── AI.png
│   │   ├── Cancer_Detection.png
│   │   ├── Lec1_Introduction_to_ML.md
│   │   ├── Price_Sqft.png
│   │   ├── lecture1.ipynb
│   │   ├── linear_regression.ipynb
│   │   ├── ml_problem.ipynb
│   │   └── python_complementary.ipynb
│   ├── lecture_02
│   │   ├── Salary_dataset.csv
│   │   ├── gradient_descent_visualisation.png
│   │   ├── kaggle
│   │   │   ├── data_description.txt
│   │   │   ├── sample_submission.csv
│   │   │   ├── test.csv
│   │   │   └── train.csv
│   │   ├── kaggle_simple_regression.ipynb
│   │   ├── lecture2.ipynb
│   │   └── linear_regression_complementary.ipynb
│   ├── lecture_03
│   │   ├── kaggle
│   │   │   ├── data_description.txt
│   │   │   ├── sample_submission.csv
│   │   │   ├── test.csv
│   │   │   └── train.csv
│   │   ├── kaggle_submission.csv
│   │   ├── lecture3.ipynb
│   │   ├── multiple_linear_regression_complementary.ipynb
│   │   └── playground-series-s3e16
│   │       ├── sample_submission.csv
│   │       ├── test.csv
│   │       └── train.csv
│   ├── lecture_04
│   │   ├── inputs_layer.png
│   │   ├── lecture4.ipynb
│   │   ├── nn.png
│   │   ├── outputs_layer.png
│   │   ├── perceptron-in-machine-learning2.png
│   │   └── perceptron.png
│   ├── lecture_05
│   │   ├── activation_functions_and_loss.ipynb
│   │   ├── back_prop_notes.pdf
│   │   ├── kaggle
│   │   │   ├── data_description.txt
│   │   │   ├── sample_submission.csv
│   │   │   ├── test.csv
│   │   │   └── train.csv
│   │   ├── lecture5.ipynb
│   │   ├── nn.png
│   │   └── relud.png
│   ├── lecture_06
│   │   ├── lecture6.ipynb
│   │   └── lecture6_optimised.ipynb
│   ├── lecture_07
│   │   ├── Convolutional_Neural_Networks_(CNNs).md
│   │   ├── DATA_MNIST
│   │   │   └── MNIST
│   │   │       └── raw
│   │   │           ├── t10k-images-idx3-ubyte
│   │   │           ├── t10k-images-idx3-ubyte.gz
│   │   │           ├── t10k-labels-idx1-ubyte
│   │   │           ├── t10k-labels-idx1-ubyte.gz
│   │   │           ├── train-images-idx3-ubyte
│   │   │           ├── train-images-idx3-ubyte.gz
│   │   │           ├── train-labels-idx1-ubyte
│   │   │           └── train-labels-idx1-ubyte.gz
│   │   ├── Images
│   │   │   ├── CNN_Structure.png
│   │   │   ├── DA1.png
│   │   │   ├── DA2.png
│   │   │   ├── DA3.png
│   │   │   ├── DA4.png
│   │   │   ├── Data_Feeding.png
│   │   │   ├── Kernel_1.png
│   │   │   ├── Kernel_2.png
│   │   │   ├── Num_example.png
│   │   │   ├── PL.png
│   │   │   ├── Padding.png
│   │   │   ├── RGB_input.png
│   │   │   ├── R_Matrix.png
│   │   │   ├── ReLU.png
│   │   │   ├── S_and_P.png
│   │   │   ├── Stride.png
│   │   │   └── Training_CNNs.png
│   │   ├── data
│   │   │   └── MNIST
│   │   │       └── raw
│   │   │           ├── t10k-images-idx3-ubyte
│   │   │           ├── t10k-images-idx3-ubyte.gz
│   │   │           ├── t10k-labels-idx1-ubyte
│   │   │           ├── t10k-labels-idx1-ubyte.gz
│   │   │           ├── train-images-idx3-ubyte
│   │   │           ├── train-images-idx3-ubyte.gz
│   │   │           ├── train-labels-idx1-ubyte
│   │   │           └── train-labels-idx1-ubyte.gz
│   │   ├── lecture7.ipynb
│   │   └── lecture7_original.ipynb
│   ├── lecture_08
│   │   └── lectue8.ipynb
│   ├── lecture_09
│   │   ├── Core_challenge.png
│   │   ├── Slot_machine.png
│   │   ├── epsilon_greedy_k_arm_bandit_problem.md
│   │   ├── lecture9.ipynb
│   │   └── ε-Greedy_Algorithm.png
│   ├── lecture_10
│   │   ├── lecture10.ipynb
│   │   └── lecture10.md
│   ├── lecture_11
│   │   ├── Practical_Example.ipynb
│   │   ├── RNNs.ipynb
│   │   ├── Sequential_Data.ipynb
│   │   └── frankenstein.txt
│   └── lecture_12
│       └── Lecture12.ipynb
└── README.md
```

<br>

## Reflection 🪞

Joining the QMUL Machine Learning Society marked the beginning of a significant personal and academic shift for me. Coming from a non-STEM background, I initially found many of the concepts—especially the mathematical foundations and programming-heavy workshops—intimidating. Concepts like gradient descent, back-propagation, and reinforcement learning were all new territory, and it was easy to feel behind peers with prior technical experience.

<br>

However, this challenge became a driving force. The society’s beginner-friendly yet rigorous progression—from foundational topics like simple and multiple linear regression to more advanced subjects such as neural networks, CNNs, RNNs, and LSTMs—helped me gradually build confidence. Each hands-on session pushed me to not just understand the what but also the how behind machine learning algorithms.

<br>

I also took the opportunity to step outside my comfort zone by learning new tools such as PyTorch, TensorFlow, and Keras—frameworks that once felt out of reach but are now part of my regular learning process. The introduction to Large Language Models (LLMs) and reinforcement learning further broadened my perspective on the diverse directions this field can offer.

<br>

This experience has not only helped me bridge the gap between my previous education and my current studies in computing, but it’s also sparked genuine curiosity. I’m still exploring what direction I’ll take—whether in research, engineering, or something entirely different—but I now feel equipped with a foundational skill set and a community that makes continued learning both accessible and rewarding.

<br>

</div>