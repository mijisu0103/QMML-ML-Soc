{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "789b7545-c7a3-481a-ac94-2fa37d8bf2ad",
   "metadata": {},
   "source": [
    "## **Review of Activation Functions**\n",
    "\n",
    "Activation functions are a crucial component in neural networks. They determine how the weighted sum of inputs and biases is transformed to introduce non-linearity, allowing the network to learn and model complex patterns. Here, we review three widely-used activation functions: ReLU, sigmoid, and softmax.\n",
    "\n",
    "### **1. Rectified Linear Unit (ReLU)**\n",
    "\n",
    "The ReLU activation function is defined as:\n",
    "\n",
    "$$\n",
    "f(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "#### Key Features:\n",
    "- **Behavior**: Outputs the input directly if it's positive; otherwise, it outputs zero.\n",
    "- **Advantages**:\n",
    "  - Efficient computation.\n",
    "  - Helps mitigate the vanishing gradient problem by keeping gradients large for positive inputs.\n",
    "- **Disadvantages**:\n",
    "  - May suffer from the \"dying ReLU\" problem, where neurons can become inactive and output zero for all inputs.\n",
    "\n",
    "#### Plot:\n",
    "ReLU is a piecewise linear function with a sharp transition at zero.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Sigmoid**\n",
    "\n",
    "The sigmoid activation function is given by:\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "#### Key Features:\n",
    "- **Behavior**: Maps inputs to a range between 0 and 1.\n",
    "- **Advantages**:\n",
    "  - Useful for models where outputs need to represent probabilities.\n",
    "  - Smooth gradients can aid optimization in shallow networks.\n",
    "- **Disadvantages**:\n",
    "  - Gradients can vanish for inputs that are far from zero.\n",
    "  - Output saturation leads to slower convergence.\n",
    "\n",
    "#### Plot:\n",
    "The sigmoid function has an S-shaped curve, asymptoting at 0 and 1.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Softmax**\n",
    "\n",
    "The softmax function is often used for the output layer in classification problems. It is defined as:\n",
    "\n",
    "$$\n",
    "f(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
    "$$\n",
    "\n",
    "#### Key Features:\n",
    "- **Behavior**: Converts logits (raw scores) into probabilities that sum to 1.\n",
    "- **Advantages**:\n",
    "  - Ideal for multi-class classification tasks.\n",
    "  - Outputs are interpretable as probabilities.\n",
    "- **Disadvantages**:\n",
    "  - Computationally expensive for large output spaces due to the exponential and summation operations.\n",
    "\n",
    "#### Plot:\n",
    "Unlike ReLU or sigmoid, the softmax function operates on vectors rather than scalar values, and its output is a probability distribution.\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison**\n",
    "\n",
    "| Function  | Range         | Applications               | Pros                              | Cons                              |\n",
    "|-----------|---------------|----------------------------|-----------------------------------|-----------------------------------|\n",
    "| **ReLU**  | [0, âˆž)        | Hidden layers in deep nets | Computationally efficient         | Dying ReLU problem                |\n",
    "| **Sigmoid** | (0, 1)       | Binary classification      | Probabilistic output              | Vanishing gradients               |\n",
    "| **Softmax** | [0, 1] (sum=1) | Multi-class classification| Probabilistic interpretation      | Computationally intensive for large outputs |\n",
    "\n",
    "Activation functions play a pivotal role in enabling neural networks to learn intricate relationships in data. Choosing the right activation function is context-dependent and can significantly impact a model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e66f77e-654a-4efa-8cee-b54de1fb2190",
   "metadata": {},
   "source": [
    "## **Categorical Cross-Entropy Loss**\n",
    "\n",
    "Categorical Cross-Entropy (CCE) is a widely used loss function for classification tasks, especially multi-class classification. It quantifies the difference between the predicted probability distribution (from the model) and the true distribution (ground truth labels).\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Softmax for Probability Distribution**\n",
    "\n",
    "To compute categorical cross-entropy, the network output is typically passed through a softmax activation function to convert logits into probabilities:\n",
    "\n",
    "$$\n",
    "f(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ x_i $ is the $ i^{th} $ logit (raw output) from the model.\n",
    "- $ \\sum_{j} e^{x_j} $ is the sum of exponentials of all logits, normalizing the probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Categorical Cross-Entropy Loss**\n",
    "\n",
    "The categorical cross-entropy loss is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{i} y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ y_i $ is the ground truth label for class $ i $ (one-hot encoded: 1 for the correct class and 0 for others).\n",
    "- $ \\hat{y}_i $ is the predicted probability for class $ i $, obtained from the softmax function.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Points:**\n",
    "\n",
    "1. **Interpretation**:\n",
    "   - The loss measures the difference between the true label distribution $ y_i $ and the predicted probabilities $ \\hat{y}_i $.\n",
    "   - For the correct class, $ y_i = 1 $, and the loss simplifies to $ -\\log(\\hat{y}_i) $, penalizing low confidence predictions for the correct class.\n",
    "\n",
    "2. **Applications**:\n",
    "   - Commonly used in multi-class classification tasks.\n",
    "   - Works well with neural networks where the output layer uses softmax activation.\n",
    "\n",
    "3. **Advantages**:\n",
    "   - Provides a probabilistic interpretation of predictions.\n",
    "   - Directly optimizes for the correct class's probability.\n",
    "\n",
    "4. **Disadvantage**:\n",
    "   - Susceptible to overfitting, especially in cases with imbalanced datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "Categorical Cross-Entropy Loss is a cornerstone in machine learning classification tasks. Its synergy with softmax activation allows models to output interpretable probabilities, while the logarithmic penalty drives the model to predict with higher confidence for the correct class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbc11eb6-7a98-42da-8750-b3f910d83a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66524096 0.09003057 0.24472847]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\" Imagine we are trying to use a Neural Network (NN) to predict whether an image\n",
    "is a cat (index 0), a dog (index 1), or a bird (index 2).\n",
    "\"\"\"\n",
    "\n",
    "output = [4, # cat\n",
    "         2,  # dog\n",
    "         3]  # bird\n",
    "\n",
    "# To convert the outputs into probabilities we apply the Softmax activation function\n",
    "\n",
    "def softmax(list):\n",
    "    exp_values = np.exp(output - np.max(output)) # Subtract max for numerical stability\n",
    "    return exp_values / np.sum(exp_values)\n",
    "\n",
    "softmax_output = softmax(output)\n",
    "print(softmax_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f772b4-cb4e-4ee6-897e-99be2f44564e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax output:\n",
      " [[0.66524096 0.09003057 0.24472847]\n",
      " [0.09962365 0.73612472 0.16425163]]\n"
     ]
    }
   ],
   "source": [
    "# Using a Class\n",
    "\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))  # Stabilize\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "        return probabilities  # Add return statement\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inputs = np.array([[4.0, 2.0, 3.0], \n",
    "                   [0.5, 2.5, 1.0]])  # Example batch input\n",
    "\n",
    "activation_softmax = Activation_Softmax()  # Create Softmax Activation\n",
    "softmax_output = activation_softmax.forward(inputs)  # Compute softmax\n",
    "print(\"Softmax output:\\n\", softmax_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67eda68e-6330-40b8-898e-7e8ecd31f70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "# Compute the CCE Loss\n",
    "\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "\n",
    "class_targets = np.array([[1, 0, 0],\n",
    "                          [0, 1, 0],\n",
    "                          [0, 1, 0]])\n",
    "\n",
    "# Another way of representing class targets\n",
    "\n",
    "class_targets2 = np.array([0,1,1])\n",
    "\n",
    "# Probabilities for target values -\n",
    "# only if categorical labels\n",
    "if len(class_targets.shape) == 1:\n",
    "    correct_confidences = softmax_outputs[\n",
    "        range(len(softmax_outputs)),\n",
    "        class_targets\n",
    "    ]\n",
    "    \n",
    "elif len(class_targets.shape) == 2:\n",
    "    correct_confidences = np.sum(\n",
    "        softmax_outputs * class_targets,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "neg_log = -np.log(correct_confidences)\n",
    "average_loss = np.mean(neg_log)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ac40c94-ddde-40e8-813f-1e814ccfd5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample losses:  [0.35667494 0.69314718 0.10536052]\n",
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        print(\"Sample losses: \", sample_losses)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped*y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "loss = loss_function.calculate(softmax_outputs, class_targets)\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
